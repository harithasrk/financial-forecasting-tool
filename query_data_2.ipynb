{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac0336c9-db56-4981-bab7-5b794326657e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Initializing Carrier RAG System...\n",
      "[CarrierRAG] Loading existing vector store...\n",
      "[CarrierRAG] Loaded existing vector store with 1370 chunks\n",
      "\n",
      "📊 Testing with sample questions...\n",
      "\n",
      "[1] What was Carrier's GAAP EPS in Q1 2024?\n",
      "Answer: According to [S1] Q1_2024.md, Carrier's GAAP EPS in Q1 2024 was $0.29.\n",
      "Sources: ['Q1_2024.md', 'Q4_2021.md', 'Q1_2024.md', 'Q4_2022.md', 'Q2_2024.md', 'Q1_2025.md', 'Q2_2024.md', 'Q4_2023.md', 'Q2_2025.md', 'Q2_2020.md', 'Q3_2023.md', 'Q3_2023.md', 'Q1_2020.md', 'Q4_2022.md', 'Q2_2024.md']\n",
      "\n",
      "[2] Using historical data, forecast Carrier's operating margin for Q4 2024\n",
      "Answer: Based on the provided context snippets, I will analyze the historical trends and patterns to forecast Carrier's operating margin for Q4 2024.\n",
      "\n",
      "From [S1] Q4_2023.md, we can see that Carrier's full-year 2023 sales were $22.1B, with organic sales growth of 3% and a 5% impact from acquisitions and divestitures. Gross margins increased 210 basis points compared to the prior year.\n",
      "\n",
      "From [S11] Q4_2023.md, we can see that GAAP operating profit was $2.3B, while adjusted operating profit increased 11% to $3.2B. Operating margin decreased due to the prior year impact of the Chubb and Toshiba Carrier-related gains.\n",
      "\n",
      "From [S12] Q3_2024.md, we can see that the Fire & Security segment is now reported as discontinued operations, making prior guidance not comparable.\n",
      "\n",
      "To forecast Carrier's operating margin for Q4 2024, I will analyze the historical trends:\n",
      "\n",
      "* In Q4 2023, GAAP operating profit was $2.3B, and adjusted operating profit increased 11% to $3.2B.\n",
      "* In Q3 2024, GAAP operating profit from continuing operations was not disclosed.\n",
      "\n",
      "Based on this analysis, I forecast Carrier's operating margin for Q4 2024 as follows:\n",
      "\n",
      "* Assuming a similar trend of organic sales growth and gross margins increase, I estimate that Carrier's adjusted operating profit will be around $3.5B to $4.0B.\n",
      "* Given the discontinued operations of the Fire & Security segment, I expect GAAP operating profit to be lower than in Q4 2023.\n",
      "\n",
      "Using these estimates, I forecast Carrier's operating margin for Q4 2024 as follows:\n",
      "\n",
      "* GAAP operating margin: around 10% to 11%, considering the impact of discontinued operations.\n",
      "* Adjusted operating margin: around 13.5% to 14.5%, assuming a similar trend of gross margins increase and organic sales growth.\n",
      "\n",
      "Please note that this is a rough estimate based on historical trends and may not reflect actual results, as there are many factors that can affect Carrier's financial performance in Q4 2024.\n",
      "Sources: ['Q4_2023.md', 'Q1_2024.md', 'Q4_2023.md', 'Q1_2025.md', 'Q1_2024.md', 'Q3_2022.md', 'Q3_2020.md', 'Q2_2024.md', 'Q2_2020.md', 'Q2_2024.md', 'Q4_2023.md', 'Q3_2024.md', 'Q4_2020.md', 'Q3_2024.md', 'Q4_2023.md']\n",
      "\n",
      "[3] Using historical performance data from all available past quarters, forecast Carrier's GAAP EPS for Q3 2025. Provide:\n",
      "        1. A point estimate and 80% confidence interval\n",
      "        2. Your analytical methodology and reasoning\n",
      "        3. Specific past quarters that most influence your projection\n",
      "        4. Key assumptions and risk factors that could affect accuracy\n",
      "        5. Seasonal patterns or trends identified in the historical data\n",
      "        6. A comparison to any available analyst consensus, if found\n",
      "Answer: Based on the provided context snippets, I will forecast Carrier's GAAP EPS for Q3 2025.\n",
      "\n",
      "**Point Estimate and 80% Confidence Interval**\n",
      "\n",
      "Using a weighted average of past quarters' GAAP EPS growth rates, I estimate Carrier's GAAP EPS for Q3 2025 to be:\n",
      "\n",
      "* Point estimate: $0.63\n",
      "* 80% confidence interval: ($0.59, $0.67)\n",
      "\n",
      "**Analytical Methodology and Reasoning**\n",
      "\n",
      "I analyzed the historical performance data from all available past quarters (Q1_2022, Q2_2022, Q3_2022, Q4_2022, Q1_2023, Q2_2023, Q3_2023, Q4_2023, Q1_2024, Q2_2024, Q3_2024, and Q4_2024) to identify trends and patterns. I then applied a weighted average of these growth rates to forecast the GAAP EPS for Q3 2025.\n",
      "\n",
      "**Specific Past Quarters that most influence my projection**\n",
      "\n",
      "The following past quarters had significant impacts on my projection:\n",
      "\n",
      "* Q1_2022: A strong quarter with a GAAP EPS of $0.84, driven by volume growth in the HVAC business and aggressive cost containment.\n",
      "* Q4_2023: A solid quarter with a GAAP EPS of $0.49, reflecting operating margin expansion and adjusted operating margin growth.\n",
      "\n",
      "**Key Assumptions and Risk Factors**\n",
      "\n",
      "Assumptions:\n",
      "\n",
      "1. Carrier's historical performance trends will continue to some extent in the future.\n",
      "2. The company will maintain its focus on cost containment and operational efficiency.\n",
      "\n",
      "Risk factors:\n",
      "\n",
      "1. Economic downturn or recession could negatively impact Carrier's sales and profitability.\n",
      "2. Unforeseen changes in market conditions, such as shifts in global demand for HVAC products, could affect the company's performance.\n",
      "\n",
      "**Seasonal Patterns or Trends Identified**\n",
      "\n",
      "Based on the historical data, I identified a seasonal pattern of higher GAAP EPS during Q4 (average growth rate: 10%) and lower GAAP EPS during Q1 (average decline rate: 5%). This trend may influence my projection for Q3 2025.\n",
      "\n",
      "**Comparison to Analyst Consensus**\n",
      "\n",
      "I did not find any available analyst consensus estimates for Carrier's GAAP EPS in Q3 2025. However, I can provide a comparison with the company's historical average quarterly growth rate (average growth rate: 6%). This comparison suggests that my point estimate of $0.63 is slightly below the historical average.\n",
      "\n",
      "Please note that this forecast is based on publicly available data and should not be considered as investment advice.\n",
      "Sources: ['Q4_2022.md', 'Q4_2023.md', 'Q1_2025.md', 'Q1_2025.md', 'Q1_2022.md', 'Q3_2021.md', 'Q2_2025.md', 'Q1_2024.md', 'Q3_2023.md', 'Q3_2020.md', 'Q3_2020.md', 'Q2_2024.md', 'Q3_2020.md', 'Q4_2023.md', 'Q1_2025.md']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import uuid\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import chromadb\n",
    "from dotenv import load_dotenv\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "class CarrierRAGSystem:\n",
    "    \"\"\"\n",
    "    Complete RAG system for Carrier earnings analysis using LangChain and Ollama\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Optional[Dict] = None):\n",
    "        \"\"\"Initialize the RAG system with configuration\"\"\"\n",
    "        load_dotenv()\n",
    "        \n",
    "        # Default configuration\n",
    "        default_config = {\n",
    "            \"chroma_path\": \"chroma_20250806\",\n",
    "            \"collection_name\": \"earnings_markdown\",\n",
    "            \"data_path\": \"/Users/haritha/hari/carrier_earnings_release/markdown_output_20250806_094639\",\n",
    "            \"ollama_host\": \"http://localhost:11434\",\n",
    "            \"embed_model\": \"nomic-embed-text\",\n",
    "            \"chat_model\": \"llama3\",\n",
    "            \"chunk_size\": 1000,\n",
    "            \"chunk_overlap\": 200,\n",
    "            \"batch_size\": 32,\n",
    "            \"clear_collection\": True\n",
    "        }\n",
    "        \n",
    "        # Merge with provided config\n",
    "        self.config = {**default_config, **(config or {})}\n",
    "        \n",
    "        # Apply environment variable overrides\n",
    "        for key, env_key in [\n",
    "            (\"chroma_path\", \"CHROMA_PATH\"),\n",
    "            (\"collection_name\", \"COLLECTION_NAME\"),\n",
    "            (\"data_path\", \"DATA_PATH\"),\n",
    "            (\"ollama_host\", \"OLLAMA_HOST\"),\n",
    "            (\"embed_model\", \"EMBED_MODEL\"),\n",
    "            (\"chat_model\", \"CHAT_MODEL\")\n",
    "        ]:\n",
    "            if env_key in os.environ:\n",
    "                self.config[key] = os.environ[env_key]\n",
    "        \n",
    "        # Set up paths\n",
    "        self.base_dir = Path.cwd()\n",
    "        self.chroma_dir = str(self.base_dir / self.config[\"chroma_path\"])\n",
    "        \n",
    "        # Initialize components\n",
    "        self.embedding_function = None\n",
    "        self.vectorstore = None\n",
    "        self.llm = None\n",
    "        \n",
    "        # Regex patterns for cleaning\n",
    "        self._base64_line = re.compile(r\"^[A-Za-z0-9+/=]{80,}$\")\n",
    "        self._data_uri = re.compile(r\"^data:image/[^;]+;base64,\", re.IGNORECASE)\n",
    "        \n",
    "        # Prompt template for financial analysis\n",
    "        self.prompt_template = \"\"\"You are an expert financial analyst specializing in Carrier Corporation earnings.\n",
    "\n",
    "Answer from the provided context snippets. For forecasting questions, analyze historical patterns and trends from the available data.\n",
    "\n",
    "Rules:\n",
    "- Quote figures exactly as written in the source material\n",
    "- After each figure or claim, include a bracketed citation like [S1] pointing to the snippet ID\n",
    "- Be concise but accurate\n",
    "- Do NOT guess or invent numbers not found in the context\n",
    "- For forecasting questions: analyze historical trends, seasonal patterns, and provide a data-driven estimate with clear reasoning\n",
    "- If insufficient data for forecasting, explain what data is missing\n",
    "- Focus on financial metrics, trends, and business performance\n",
    "\n",
    "Context Snippets:\n",
    "{context}\n",
    "\n",
    "---\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    def log(self, msg: str) -> None:\n",
    "        \"\"\"Logging utility\"\"\"\n",
    "        print(f\"[CarrierRAG] {msg}\")\n",
    "\n",
    "    def clean_markdown(self, text: str) -> str:\n",
    "        \"\"\"Clean markdown text by removing base64 images and overly long lines\"\"\"\n",
    "        cleaned_lines = []\n",
    "        for line in text.splitlines():\n",
    "            # Skip base64 images, data URIs, and very long lines\n",
    "            if (self._data_uri.search(line) or \n",
    "                self._base64_line.match(line) or \n",
    "                len(line) > 1200):\n",
    "                continue\n",
    "            cleaned_lines.append(line)\n",
    "        return \"\\n\".join(cleaned_lines).strip()\n",
    "\n",
    "    def ensure_fresh_chroma_dir(self) -> None:\n",
    "        \"\"\"Prepare a clean Chroma directory\"\"\"\n",
    "        if os.path.exists(self.chroma_dir):\n",
    "            shutil.rmtree(self.chroma_dir, ignore_errors=True)\n",
    "        os.makedirs(self.chroma_dir, exist_ok=True)\n",
    "        \n",
    "        # Test write access\n",
    "        test_file = Path(self.chroma_dir) / \".write_test\"\n",
    "        with open(test_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"ok\")\n",
    "        test_file.unlink(missing_ok=True)\n",
    "        self.log(f\"Persistence directory ready: {self.chroma_dir}\")\n",
    "\n",
    "    def load_documents(self) -> List[Document]:\n",
    "        \"\"\"Load and clean markdown documents\"\"\"\n",
    "        data_path = Path(self.config[\"data_path\"])\n",
    "        if not data_path.exists():\n",
    "            raise FileNotFoundError(f\"Data path not found: {data_path}\")\n",
    "        \n",
    "        md_files = list(data_path.glob(\"*.md\"))\n",
    "        if not md_files:\n",
    "            raise RuntimeError(f\"No .md files found in: {data_path}\")\n",
    "        \n",
    "        documents = []\n",
    "        for file_path in sorted(md_files):\n",
    "            try:\n",
    "                loader = TextLoader(str(file_path), encoding=\"utf-8\")\n",
    "                docs = loader.load()\n",
    "                for doc in docs:\n",
    "                    doc.page_content = self.clean_markdown(doc.page_content)\n",
    "                    documents.append(doc)\n",
    "            except Exception as e:\n",
    "                self.log(f\"Warning: Failed to load {file_path}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        self.log(f\"Loaded {len(documents)} markdown files from {data_path}\")\n",
    "        return documents\n",
    "\n",
    "    def split_documents(self, documents: List[Document]) -> List[Document]:\n",
    "        \"\"\"Split documents into chunks\"\"\"\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=self.config[\"chunk_size\"],\n",
    "            chunk_overlap=self.config[\"chunk_overlap\"],\n",
    "            length_function=len,\n",
    "            add_start_index=True,\n",
    "        )\n",
    "        \n",
    "        chunks = splitter.split_documents(documents)\n",
    "        self.log(f\"Split {len(documents)} documents into {len(chunks)} chunks\")\n",
    "        \n",
    "        # Show preview\n",
    "        if chunks:\n",
    "            preview_chunk = chunks[min(5, len(chunks) - 1)]\n",
    "            print(f\"Preview chunk: {preview_chunk.page_content[:300]}...\")\n",
    "            print(f\"Metadata: {preview_chunk.metadata}\")\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def get_embedding_function(self):\n",
    "        \"\"\"Get the embedding function\"\"\"\n",
    "        if self.embedding_function is None:\n",
    "            self.embedding_function = OllamaEmbeddings(\n",
    "                model=self.config[\"embed_model\"],\n",
    "                base_url=self.config[\"ollama_host\"]\n",
    "            )\n",
    "        return self.embedding_function\n",
    "\n",
    "    def build_vectorstore(self, force_rebuild: bool = False) -> None:\n",
    "        \"\"\"Build or load the vector store\"\"\"\n",
    "        if not force_rebuild and os.path.exists(self.chroma_dir):\n",
    "            self.log(\"Loading existing vector store...\")\n",
    "            try:\n",
    "                self.vectorstore = self._load_existing_vectorstore()\n",
    "                count = self.vectorstore._collection.count()\n",
    "                self.log(f\"Loaded existing vector store with {count} chunks\")\n",
    "                return\n",
    "            except Exception as e:\n",
    "                self.log(f\"Failed to load existing store: {e}. Rebuilding...\")\n",
    "        \n",
    "        # Build new vector store\n",
    "        self.log(\"Building new vector store...\")\n",
    "        self.ensure_fresh_chroma_dir()\n",
    "        \n",
    "        # Load and process documents\n",
    "        documents = self.load_documents()\n",
    "        chunks = self.split_documents(documents)\n",
    "        \n",
    "        # Create vector store with batched embedding\n",
    "        self._create_vectorstore_with_batching(chunks)\n",
    "\n",
    "    def _load_existing_vectorstore(self) -> Chroma:\n",
    "        \"\"\"Load existing vector store\"\"\"\n",
    "        client = chromadb.PersistentClient(path=self.chroma_dir)\n",
    "        return Chroma(\n",
    "            client=client,\n",
    "            collection_name=self.config[\"collection_name\"],\n",
    "            embedding_function=self.get_embedding_function()\n",
    "        )\n",
    "\n",
    "    def _create_vectorstore_with_batching(self, chunks: List[Document]) -> None:\n",
    "        \"\"\"Create vector store with batched embedding for efficiency\"\"\"\n",
    "        texts = [chunk.page_content for chunk in chunks]\n",
    "        metadatas = [chunk.metadata for chunk in chunks]\n",
    "        ids = [str(uuid.uuid4()) for _ in texts]\n",
    "        \n",
    "        total = len(texts)\n",
    "        batch_size = self.config[\"batch_size\"]\n",
    "        total_batches = (total + batch_size - 1) // batch_size\n",
    "        \n",
    "        # Get embedding function\n",
    "        embedding_fn = self.get_embedding_function()\n",
    "        \n",
    "        # Batch embedding with progress tracking\n",
    "        self.log(f\"Embedding {total} chunks in {total_batches} batches of {batch_size}...\")\n",
    "        embeddings = []\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        for batch_idx, i in enumerate(range(0, total, batch_size), start=1):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            t0 = time.time()\n",
    "            \n",
    "            try:\n",
    "                batch_embeddings = embedding_fn.embed_documents(batch_texts)\n",
    "                embeddings.extend(batch_embeddings)\n",
    "            except Exception as e:\n",
    "                self.log(f\"Error in batch {batch_idx}: {e}\")\n",
    "                raise\n",
    "            \n",
    "            # Progress tracking\n",
    "            dt = time.time() - t0\n",
    "            elapsed = datetime.now() - start_time\n",
    "            avg_time = elapsed / batch_idx\n",
    "            eta = avg_time * (total_batches - batch_idx)\n",
    "            \n",
    "            print(f\"[{batch_idx:>3}/{total_batches}] +{len(batch_texts):>3} in {dt:.2f}s | \"\n",
    "                  f\"done {i + len(batch_texts):>5}/{total} | \"\n",
    "                  f\"ETA {str(timedelta(seconds=int(eta.total_seconds())))}\")\n",
    "        \n",
    "        # Create Chroma collection\n",
    "        client = chromadb.PersistentClient(path=self.chroma_dir)\n",
    "        collection = client.get_or_create_collection(\n",
    "            name=self.config[\"collection_name\"],\n",
    "            metadata={\"hnsw:space\": \"cosine\"}\n",
    "        )\n",
    "        \n",
    "        # Clear if requested\n",
    "        if self.config[\"clear_collection\"]:\n",
    "            try:\n",
    "                collection.delete(where={})\n",
    "                self.log(\"Cleared existing collection contents\")\n",
    "            except Exception as e:\n",
    "                self.log(f\"Could not clear collection: {e}\")\n",
    "        \n",
    "        # Batch upsert\n",
    "        for i in range(0, total, batch_size):\n",
    "            end_idx = min(i + batch_size, total)\n",
    "            collection.upsert(\n",
    "                ids=ids[i:end_idx],\n",
    "                documents=texts[i:end_idx],\n",
    "                metadatas=metadatas[i:end_idx],\n",
    "                embeddings=embeddings[i:end_idx]\n",
    "            )\n",
    "        \n",
    "        # Create LangChain wrapper\n",
    "        self.vectorstore = Chroma(\n",
    "            client=client,\n",
    "            collection_name=self.config[\"collection_name\"],\n",
    "            embedding_function=embedding_fn\n",
    "        )\n",
    "        \n",
    "        final_count = collection.count()\n",
    "        self.log(f\"✅ Created vector store with {final_count} chunks\")\n",
    "\n",
    "    def get_llm(self):\n",
    "        \"\"\"Get the chat LLM\"\"\"\n",
    "        if self.llm is None:\n",
    "            self.llm = ChatOllama(\n",
    "                model=self.config[\"chat_model\"],\n",
    "                base_url=self.config[\"ollama_host\"],\n",
    "                temperature=0,  # Deterministic for financial analysis\n",
    "                num_ctx=4096    # Context window\n",
    "            )\n",
    "        return self.llm\n",
    "\n",
    "    def _doc_key(self, doc) -> Tuple[str, int]:\n",
    "        \"\"\"Create unique key for document deduplication\"\"\"\n",
    "        source = doc.metadata.get(\"source\", \"\")\n",
    "        start_index = doc.metadata.get(\"start_index\", -1)\n",
    "        return (source, start_index)\n",
    "\n",
    "    def _filename(self, doc) -> str:\n",
    "        \"\"\"Extract filename from document metadata\"\"\"\n",
    "        source = doc.metadata.get(\"source\", \"\")\n",
    "        return Path(source).name if source else \"unknown.md\"\n",
    "\n",
    "    def _build_context_and_sources(self, docs: List) -> Tuple[str, Dict[str, str]]:\n",
    "        \"\"\"Build numbered context with source mapping\"\"\"\n",
    "        context_lines = []\n",
    "        sources_map = {}\n",
    "        \n",
    "        for i, doc in enumerate(docs, start=1):\n",
    "            source_id = f\"S{i}\"\n",
    "            snippet = doc.page_content.strip()\n",
    "            # Trim very long snippets to reduce noise\n",
    "            if len(snippet) > 1200:\n",
    "                snippet = snippet[:1200] + \"...\"\n",
    "            \n",
    "            filename = self._filename(doc)\n",
    "            sources_map[source_id] = filename\n",
    "            context_lines.append(f\"[{source_id}] {filename}\\n{snippet}\\n\")\n",
    "        \n",
    "        return \"\\n\".join(context_lines), sources_map\n",
    "\n",
    "    def expand_query_for_forecasting(self, query: str) -> List[str]:\n",
    "        \"\"\"Generate additional search queries for forecasting questions\"\"\"\n",
    "        base_queries = [query]\n",
    "        \n",
    "        # Check if this is a forecasting question\n",
    "        forecast_indicators = ['forecast', 'predict', 'estimate', 'expect', 'outlook', 'guidance', 'future', 'next', 'upcoming']\n",
    "        is_forecast = any(indicator in query.lower() for indicator in forecast_indicators)\n",
    "        \n",
    "        if is_forecast:\n",
    "            # Extract key terms\n",
    "            quarters = ['Q1', 'Q2', 'Q3', 'Q4', 'first quarter', 'second quarter', 'third quarter', 'fourth quarter']\n",
    "            metrics = ['EPS', 'earnings', 'revenue', 'income', 'profit', 'margin', 'growth']\n",
    "            \n",
    "            # Add historical queries\n",
    "            for quarter in quarters:\n",
    "                if quarter.lower() in query.lower():\n",
    "                    base_queries.extend([\n",
    "                        f\"{quarter} earnings historical performance\",\n",
    "                        f\"{quarter} EPS previous years\",\n",
    "                        f\"{quarter} financial results trends\"\n",
    "                    ])\n",
    "                    break\n",
    "            \n",
    "            for metric in metrics:\n",
    "                if metric.lower() in query.lower():\n",
    "                    base_queries.extend([\n",
    "                        f\"{metric} quarterly trends\",\n",
    "                        f\"historical {metric} performance\",\n",
    "                        f\"{metric} year over year\"\n",
    "                    ])\n",
    "                    break\n",
    "        \n",
    "        return base_queries\n",
    "\n",
    "    def retrieve_context(\n",
    "        self, \n",
    "        query: str, \n",
    "        k_primary: int = 12, \n",
    "        k_mmr: int = 8, \n",
    "        fetch_k: int = 30, \n",
    "        min_score: float = 0.4\n",
    "    ) -> Tuple[str, Dict[str, str], List]:\n",
    "        \"\"\"Enhanced retrieval with query expansion for forecasting\"\"\"\n",
    "        if not self.vectorstore:\n",
    "            raise RuntimeError(\"Vector store not initialized. Call build_vectorstore() first.\")\n",
    "        \n",
    "        # Expand query for better forecasting retrieval\n",
    "        search_queries = self.expand_query_for_forecasting(query)\n",
    "        all_docs = []\n",
    "        seen_keys = set()\n",
    "        \n",
    "        # Search with multiple queries\n",
    "        for search_query in search_queries:\n",
    "            # Primary retrieval with relevance scores\n",
    "            try:\n",
    "                primary_results = self.vectorstore.similarity_search_with_relevance_scores(\n",
    "                    search_query, k=k_primary\n",
    "                )\n",
    "                \n",
    "                # Add primary results (with score filtering)\n",
    "                for doc, score in primary_results:\n",
    "                    if score is None or score >= min_score:\n",
    "                        key = self._doc_key(doc)\n",
    "                        if key not in seen_keys:\n",
    "                            seen_keys.add(key)\n",
    "                            all_docs.append(doc)\n",
    "                \n",
    "                # Diversity retrieval with MMR\n",
    "                mmr_docs = self.vectorstore.max_marginal_relevance_search(\n",
    "                    search_query, k=k_mmr, fetch_k=fetch_k\n",
    "                )\n",
    "                \n",
    "                # Add MMR results (for diversity)\n",
    "                for doc in mmr_docs:\n",
    "                    key = self._doc_key(doc)\n",
    "                    if key not in seen_keys:\n",
    "                        seen_keys.add(key)\n",
    "                        all_docs.append(doc)\n",
    "                        \n",
    "            except Exception as e:\n",
    "                self.log(f\"Warning: Error in retrieval for query '{search_query}': {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not all_docs:\n",
    "            return \"\", {}, []\n",
    "        \n",
    "        # Limit total documents to avoid token overflow\n",
    "        max_docs = 15\n",
    "        if len(all_docs) > max_docs:\n",
    "            all_docs = all_docs[:max_docs]\n",
    "        \n",
    "        context_text, sources_map = self._build_context_and_sources(all_docs)\n",
    "        return context_text, sources_map, all_docs\n",
    "\n",
    "    def answer_query(self, question: str, **retrieval_kwargs) -> Dict[str, any]:\n",
    "        \"\"\"Answer a query using the RAG system\"\"\"\n",
    "        # Retrieve context\n",
    "        context, sources_map, docs = self.retrieve_context(question, **retrieval_kwargs)\n",
    "        \n",
    "        if not context:\n",
    "            return {\n",
    "                \"answer\": \"No relevant information found. Try rephrasing your question or adjusting search parameters.\",\n",
    "                \"sources\": {},\n",
    "                \"context_docs\": 0\n",
    "            }\n",
    "        \n",
    "        # Create prompt and get LLM response\n",
    "        prompt = ChatPromptTemplate.from_template(self.prompt_template)\n",
    "        messages = prompt.format_messages(context=context, question=question)\n",
    "        \n",
    "        llm = self.get_llm()\n",
    "        response = llm.invoke(messages)\n",
    "        answer = getattr(response, \"content\", str(response))\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"sources\": sources_map,\n",
    "            \"context_docs\": len(docs),\n",
    "            \"question\": question\n",
    "        }\n",
    "\n",
    "    def interactive_query(self):\n",
    "        \"\"\"Interactive query loop\"\"\"\n",
    "        print(\"\\n🔍 Carrier RAG System - Interactive Mode\")\n",
    "        print(\"Type 'quit' to exit, 'rebuild' to rebuild the index\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                question = input(\"\\n📊 Ask about Carrier's earnings: \").strip()\n",
    "                \n",
    "                if question.lower() in ['quit', 'exit', 'q']:\n",
    "                    print(\"Goodbye! 👋\")\n",
    "                    break\n",
    "                \n",
    "                if question.lower() == 'rebuild':\n",
    "                    print(\"Rebuilding vector store...\")\n",
    "                    self.build_vectorstore(force_rebuild=True)\n",
    "                    continue\n",
    "                \n",
    "                if not question:\n",
    "                    continue\n",
    "                \n",
    "                # Get answer\n",
    "                result = self.answer_query(question)\n",
    "                \n",
    "                # Display results\n",
    "                print(f\"\\n--- Answer ---\")\n",
    "                print(result[\"answer\"])\n",
    "                \n",
    "                print(f\"\\n--- Sources ({result['context_docs']} documents) ---\")\n",
    "                for source_id, filename in result[\"sources\"].items():\n",
    "                    print(f\"{source_id}: {filename}\")\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\n\\nGoodbye! 👋\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "# Convenience functions for backward compatibility\n",
    "def build_database(config: Optional[Dict] = None):\n",
    "    \"\"\"Build the vector database\"\"\"\n",
    "    rag = CarrierRAGSystem(config)\n",
    "    rag.build_vectorstore(force_rebuild=True)\n",
    "    return rag\n",
    "\n",
    "def query_database(question: str, config: Optional[Dict] = None, **kwargs):\n",
    "    \"\"\"Query the existing database\"\"\"\n",
    "    rag = CarrierRAGSystem(config)\n",
    "    rag.build_vectorstore(force_rebuild=False)\n",
    "    return rag.answer_query(question, **kwargs)\n",
    "\n",
    "\n",
    "# Example usage and testing\n",
    "if __name__ == \"__main__\":\n",
    "    # Example 1: Direct usage\n",
    "    print(\"🚀 Initializing Carrier RAG System...\")\n",
    "    \n",
    "    # Custom configuration (optional)\n",
    "    custom_config = {\n",
    "        \"chunk_size\": 1000,\n",
    "        \"chunk_overlap\": 200,\n",
    "        \"chat_model\": \"llama3\",  # or \"mistral\", \"codellama\", etc.\n",
    "        \"embed_model\": \"nomic-embed-text\"  # or \"mxbai-embed-large\"\n",
    "    }\n",
    "    \n",
    "    # Initialize system\n",
    "    rag_system = CarrierRAGSystem(custom_config)\n",
    "    \n",
    "    # Build or load vector store\n",
    "    rag_system.build_vectorstore()\n",
    "    \n",
    "    # Example queries\n",
    "    \n",
    "    test_questions = [\n",
    "        \"What was Carrier's GAAP EPS in Q1 2024?\",\n",
    "        \"Using historical data, forecast Carrier's operating margin for Q4 2024\",\n",
    "        \"\"\"Using historical performance data from all available past quarters, forecast Carrier's GAAP EPS for Q3 2025. Provide:\n",
    "        1. A point estimate and 80% confidence interval\n",
    "        2. Your analytical methodology and reasoning\n",
    "        3. Specific past quarters that most influence your projection\n",
    "        4. Key assumptions and risk factors that could affect accuracy\n",
    "        5. Seasonal patterns or trends identified in the historical data\n",
    "        6. A comparison to any available analyst consensus, if found\"\"\",\n",
    "        \"What is a reasonable forecast for Carrier's GAAP EPS in Q2 2025 based on Q2 performance in previous years?\",\n",
    "        \"Using historical performance data from all available past quarters, forecast Carrier's GAAP EPS for Q3 2025. Provide your reasoning and reference specific past quarters that influence the projection.\",\n",
    "        \"What are the main business segments of Carrier?\",\n",
    "        \"Show me Carrier's Q2 EPS results for the last 3-4 years\",\n",
    "        \"What was Carrier's Q2 2024 GAAP EPS and adjusted EPS?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n📊 Testing with sample questions...\")\n",
    "    for i, question in enumerate(test_questions[:3], 1):  # Test first 3 questions\n",
    "        print(f\"\\n[{i}] {question}\")\n",
    "        result = rag_system.answer_query(question)\n",
    "        print(f\"Answer: {result['answer']}\")\n",
    "        print(f\"Sources: {list(result['sources'].values())}\")\n",
    "    \n",
    "    # Uncomment the next line for interactive mode\n",
    "    # rag_system.interactive_query()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
